{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NUM_WORDS = 50000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),'rb') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 622 texts.\n"
     ]
    }
   ],
   "source": [
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "texts = []  # list of text samples\n",
    "labels_index = {'negative':0,'positive':1,'neutral':2}  # dictionary mapping label name to numeric id\n",
    "\n",
    "labels = []  # list of label ids\n",
    "file_list = os.listdir(TEXT_DATA_DIR)\n",
    "for file in file_list:\n",
    "    with open('data/' + file, 'r') as f:\n",
    "        transcripts = json.load(f)\n",
    "        texts.extend(transcripts['text'].values())\n",
    "        labels.extend(transcripts['sentiment'].values())\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    labels[i] = labels_index[labels[i]]\n",
    "\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3710 unique tokens.\n",
      "Shape of data tensor: (622, 100)\n",
      "Shape of label tensor: (622, 3)\n"
     ]
    }
   ],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into a training set and a validation set\n",
    "indices = np.arange(data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "data = data[indices]\n",
    "labels = labels[indices]\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_train = data[:-num_validation_samples]\n",
    "y_train = labels[:-num_validation_samples]\n",
    "x_val = data[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 498 samples, validate on 124 samples\n",
      "Epoch 1/10\n",
      "498/498 [==============================] - 1s 2ms/step - loss: 1.0973 - acc: 0.4076 - val_loss: 1.0951 - val_acc: 0.4839\n",
      "Epoch 2/10\n",
      "498/498 [==============================] - 1s 1ms/step - loss: 1.0938 - acc: 0.5161 - val_loss: 1.0917 - val_acc: 0.4839\n",
      "Epoch 3/10\n",
      "498/498 [==============================] - 1s 1ms/step - loss: 1.0905 - acc: 0.5161 - val_loss: 1.0884 - val_acc: 0.4839\n",
      "Epoch 4/10\n",
      "498/498 [==============================] - 1s 1ms/step - loss: 1.0872 - acc: 0.5161 - val_loss: 1.0852 - val_acc: 0.4839\n",
      "Epoch 5/10\n",
      "498/498 [==============================] - 1s 1ms/step - loss: 1.0840 - acc: 0.5161 - val_loss: 1.0820 - val_acc: 0.4839\n",
      "Epoch 6/10\n",
      "498/498 [==============================] - 1s 1ms/step - loss: 1.0808 - acc: 0.5161 - val_loss: 1.0790 - val_acc: 0.4839\n",
      "Epoch 7/10\n",
      "498/498 [==============================] - 1s 1ms/step - loss: 1.0778 - acc: 0.5161 - val_loss: 1.0760 - val_acc: 0.4839\n",
      "Epoch 8/10\n",
      "498/498 [==============================] - 1s 1ms/step - loss: 1.0748 - acc: 0.5161 - val_loss: 1.0731 - val_acc: 0.4839\n",
      "Epoch 9/10\n",
      "498/498 [==============================] - 1s 1ms/step - loss: 1.0719 - acc: 0.5161 - val_loss: 1.0703 - val_acc: 0.4839\n",
      "Epoch 10/10\n",
      "498/498 [==============================] - 1s 1ms/step - loss: 1.0691 - acc: 0.5161 - val_loss: 1.0675 - val_acc: 0.4839\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a580072be0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(128, 3, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = MaxPooling1D(3)(x)\n",
    "x = Conv1D(128, 3, activation='relu')(x)\n",
    "x = GlobalMaxPooling1D()(x)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "preds = Dense(len(labels_index), activation='softmax')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 100, 100)          371100    \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 98, 128)           38528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 30, 128)           49280     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 8, 128)            49280     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 516,639\n",
      "Trainable params: 145,539\n",
      "Non-trainable params: 371,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,  120,  116,\n",
       "        2084,    3, 1609, 1420,   33,   92,  933,    7,  515,   76,  342,\n",
       "          12,  116,  103,  722, 3116, 3117, 3118,    2,  309, 3119, 3120,\n",
       "        1863, 2093,    2, 3121,  958,   14,   59,   14,    8,   76,  312,\n",
       "        1868,   12,    1,  558,  244, 2094,   49,   69,  545,  957,  839,\n",
       "         587,   87,    9,   73,   80, 1004,    4, 1566,   17,  260,    1,\n",
       "         479],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    2,   75,   69,  778,    1,  709,\n",
       "           4,  227, 2383,   87,    1,  774,  799,  190,    7,  132,  124,\n",
       "         355,    5,   40,  378,  119,  790, 1178, 1788, 2384,   85,  632,\n",
       "         189,    2,  408,    6, 1179,   87,   40,  897,  227,   21,  406,\n",
       "          95],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  107,    2, 3318, 1612,    4,  573,  566,   81,    2,\n",
       "        1175,   11,   77, 2064,   16,  201,    6,   54,  140,    4,    9,\n",
       "        1272,   48,  148,    2,  448,    2,   64,   32,   10,   37,    5,\n",
       "          77, 1501,   21,  265,   21,   77,  511,  902,   21,    2,  314,\n",
       "           6,   67, 2152,   87,    1,   41,   21,   14,   59,   96,   67,\n",
       "           4,    1,  251,  492,  754,  492,    2,  275,  492,    7,   69,\n",
       "         614],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,   27,  180, 1170,  219,  656,\n",
       "          51,   75,   10,  168,    3,    6,  308,  577,    2,  630,   32,\n",
       "         252,   54,  373,   23,    1,  665,  834,    2,   80,  577,   13,\n",
       "          84,   16,  129,  132,    3,   37,    1,  280,    4,    1, 1904,\n",
       "           4,  143,  316,    4,    6,   16,  102,    3,   29,   45,  592,\n",
       "          13,  746,  114,   78,  436,    8,  248,  287,    6,  106,   72,\n",
       "           8,  666,  600, 2168,   19,   39,   29,    8,  666,  600, 2168,\n",
       "        3395],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    1,  960,  176,\n",
       "           7,   37,  272,    7,   39,  100,    3,   37,   21,    5,  139,\n",
       "         271,    5,  344,  205,   49,  648,  311,  267,    7,  132,   99,\n",
       "         276,    2,  271,    3,   29,  882, 1263,    5,  846,  141,   14,\n",
       "           7,  151,  132,    3,   28,  124,  375,  939,  793,    2, 1569,\n",
       "           7,  100,    3,  132,    1,  475,   26,  846,  141,  603,  518,\n",
       "         146,    3,   29,  882, 1630,    1,   76,  221,  222,  959,  518,\n",
       "         146]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_prob = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707],\n",
       "       [0.3050786 , 0.3562744 , 0.33864707]], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_classes = y_prob.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_index_2 = {0:'negative',1:'positive',2:'neutral'} \n",
    "def pred_vec_to_lebal(vecs,labels_index_2):\n",
    "    indices = [np.where(r==1)[0][0] for r in vecs]\n",
    "    labels = [labels_index_2[i] for i in indices]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_labels = pred_vec_to_lebal(y_val,labels_index_2)\n",
    "y_classes_labels = pred_vec_to_lebal(y_classes,labels_index_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         |  n     p |\n",
      "         |  e  n  o |\n",
      "         |  g  e  s |\n",
      "         |  a  u  i |\n",
      "         |  t  t  t |\n",
      "         |  i  r  i |\n",
      "         |  v  a  v |\n",
      "         |  e  l  e |\n",
      "---------+----------+\n",
      "negative |<11> .  . |\n",
      " neutral | 53 <.> . |\n",
      "positive | 60  . <.>|\n",
      "---------+----------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "print(ConfusionMatrix(y_val_labels, y_classes_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
