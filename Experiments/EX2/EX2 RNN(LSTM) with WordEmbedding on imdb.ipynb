{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D, LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "MAX_SEQUENCE_LENGTH = 800\n",
    "MAX_NUM_WORDS = 500000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),'rb') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "#a = (x_train, y_train), b = (x_test, y_test)\n",
    "a,b = imdb.load_data(path=\"imdb.npz\",\n",
    "                     num_words=None,\n",
    "                     skip_top=10,\n",
    "                     maxlen=MAX_SEQUENCE_LENGTH,\n",
    "                     seed=113,\n",
    "                     start_char=1,\n",
    "                     oov_char=2,\n",
    "                     index_from=3)\n",
    "\n",
    "x_train = a[0]\n",
    "y_train = a[1]\n",
    "x_test = b[0]\n",
    "y_test = b[1]\n",
    "\n",
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 454., 1826., 3691., 5577., 3504., 2246., 1696., 1250., 1000.,\n",
       "         773.,  633.,  509.,  418.,  328.,  287.,  227.,  172.,  155.,\n",
       "         140.,  114.]),\n",
       " array([  9. ,  48.5,  88. , 127.5, 167. , 206.5, 246. , 285.5, 325. ,\n",
       "        364.5, 404. , 443.5, 483. , 522.5, 562. , 601.5, 641. , 680.5,\n",
       "        720. , 759.5, 799. ]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plot the lengths distribution when the max len set as 1000\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "p_lengths = []\n",
    "for p in x_train:\n",
    "    p_lengths.append(len(p))\n",
    "\n",
    "plt.hist(p_lengths,bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25000, 800)\n",
      "Shape of label tensor: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "x_train = pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y_train = to_categorical(np.asarray(y_train))\n",
    "y_test = to_categorical(np.asarray(y_test))\n",
    "\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of label tensor:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 4\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 800, 100)          8858800   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 50)                30200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 102       \n",
      "=================================================================\n",
      "Total params: 8,889,102\n",
      "Trainable params: 30,302\n",
      "Non-trainable params: 8,858,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# build RNN model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 24067 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 282s 11ms/step - loss: 0.6933 - acc: 0.4972 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 281s 11ms/step - loss: 0.6932 - acc: 0.4986 - val_loss: 0.6932 - val_acc: 0.5002\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 270s 11ms/step - loss: 0.6932 - acc: 0.4946 - val_loss: 0.6932 - val_acc: 0.5002\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 278s 11ms/step - loss: 0.6932 - acc: 0.5014 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 274s 11ms/step - loss: 0.6932 - acc: 0.4998 - val_loss: 0.6932 - val_acc: 0.5002\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 286s 11ms/step - loss: 0.6932 - acc: 0.4974 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 265s 11ms/step - loss: 0.6932 - acc: 0.5018 - val_loss: 0.6932 - val_acc: 0.4998\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 276s 11ms/step - loss: 0.6932 - acc: 0.4946 - val_loss: 0.6932 - val_acc: 0.5002\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 289s 12ms/step - loss: 0.6932 - acc: 0.5054 - val_loss: 0.6932 - val_acc: 0.4998\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 299s 12ms/step - loss: 0.6932 - acc: 0.4978 - val_loss: 0.6932 - val_acc: 0.5002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x255056e8358>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('EX2_RNN(LSTM)_with_WordEmbedding_trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('EX2_RNN(LSTM)_with_WordEmbedding_trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "transcripts_x = []\n",
    "transcripts_y = []\n",
    "\n",
    "file_list = os.listdir('data')\n",
    "for file in file_list:\n",
    "    with open('data/' + file, 'r') as f:\n",
    "        transcripts = json.load(f)\n",
    "        transcripts_x.extend(transcripts['text'].values())\n",
    "        transcripts_y.extend(transcripts['sentiment'].values())\n",
    "        \n",
    "#build a pandas df and drop all neutral rows\n",
    "d = {'review': transcripts_x, 'label': transcripts_y}\n",
    "df_transcripts = pd.DataFrame(data=d)\n",
    "\n",
    "#drop all neutral rows\n",
    "df_transcripts = df_transcripts[df_transcripts['label'] != 'neutral']\n",
    "\n",
    "#replace positive and negative to pos and neg\n",
    "df_transcripts['label'] = df_transcripts['label'].str.replace('positive','1')\n",
    "df_transcripts['label'] = df_transcripts['label'].str.replace('negative','0')\n",
    "\n",
    "transcripts_x = df_transcripts['review'].tolist()\n",
    "transcripts_y = df_transcripts['label'].astype('int32').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenize(text):\n",
    "    tokens = text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ')\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_x_tokens = []\n",
    "for t in transcripts_x:\n",
    "    transcripts_x_tokens.append(my_tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_x_index = []\n",
    "for t_tokens in transcripts_x_tokens:\n",
    "    t_index = []\n",
    "    for t in t_tokens:\n",
    "        #get index from word_index for current token\n",
    "        try:\n",
    "            i = word_index[t] \n",
    "        except KeyError:\n",
    "            i = 0\n",
    "        t_index.append(i)\n",
    "    transcripts_x_index.append(t_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_x_index = pad_sequences(transcripts_x_index, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "transcripts_y = to_categorical(np.asarray(transcripts_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of data tensor:', transcripts_x_index.shape)\n",
    "print('Shape of label tensor:', transcripts_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_prob = model.predict(transcripts_x_index)\n",
    "y_classes = y_prob.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_index_2 = {0:'negative',1:'positive',2:'neutral'} \n",
    "def pred_vec_to_lebal(vecs,labels_index_2):\n",
    "    indices = [np.where(r==1)[0][0] for r in vecs]\n",
    "    labels = [labels_index_2[i] for i in indices]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_labels = pred_vec_to_lebal(transcripts_y,labels_index_2)\n",
    "y_classes_labels = pred_vec_to_lebal(y_classes,labels_index_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "print(ConfusionMatrix(y_val_labels, y_classes_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
