{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.twitter.27B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "MAX_SEQUENCE_LENGTH = 800\n",
    "MAX_NUM_WORDS = 500000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.twitter.27B.25d.txt'),'rb') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "#a = (x_train, y_train), b = (x_test, y_test)\n",
    "a,b = imdb.load_data(path=\"imdb.npz\",\n",
    "                     num_words=None,\n",
    "                     skip_top=10,\n",
    "                     maxlen=MAX_SEQUENCE_LENGTH,\n",
    "                     seed=113,\n",
    "                     start_char=1,\n",
    "                     oov_char=2,\n",
    "                     index_from=3)\n",
    "\n",
    "x_train = a[0]\n",
    "y_train = a[1]\n",
    "x_test = b[0]\n",
    "y_test = b[1]\n",
    "\n",
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 454., 1826., 3691., 5577., 3504., 2246., 1696., 1250., 1000.,\n",
       "         773.,  633.,  509.,  418.,  328.,  287.,  227.,  172.,  155.,\n",
       "         140.,  114.]),\n",
       " array([  9. ,  48.5,  88. , 127.5, 167. , 206.5, 246. , 285.5, 325. ,\n",
       "        364.5, 404. , 443.5, 483. , 522.5, 562. , 601.5, 641. , 680.5,\n",
       "        720. , 759.5, 799. ]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plot the lengths distribution when the max len set as 1000\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "p_lengths = []\n",
    "for p in x_train:\n",
    "    p_lengths.append(len(p))\n",
    "\n",
    "plt.hist(p_lengths,bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25000, 800)\n",
      "Shape of label tensor: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "x_train = pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y_train = to_categorical(np.asarray(y_train))\n",
    "y_test = to_categorical(np.asarray(y_test))\n",
    "\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of label tensor:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 4\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 800, 100)          8858800   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 8,860,450\n",
      "Trainable params: 1,650\n",
      "Non-trainable params: 8,858,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 24067 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 10s 412us/step - loss: 0.6932 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.5002\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 9s 358us/step - loss: 0.6932 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 9s 351us/step - loss: 0.6932 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 9s 347us/step - loss: 0.6932 - acc: 0.4990 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 10s 381us/step - loss: 0.6932 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.5002\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 9s 362us/step - loss: 0.6932 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 8s 334us/step - loss: 0.6932 - acc: 0.4977 - val_loss: 0.6932 - val_acc: 0.5002\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 9s 356us/step - loss: 0.6932 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 9s 346us/step - loss: 0.6932 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 9s 345us/step - loss: 0.6932 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.4998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20333aff7f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('EX2_WordEmbedding_twitter_trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "transcripts_x = []\n",
    "transcripts_y = []\n",
    "\n",
    "file_list = os.listdir('data')\n",
    "for file in file_list:\n",
    "    with open('data/' + file, 'r') as f:\n",
    "        transcripts = json.load(f)\n",
    "        transcripts_x.extend(transcripts['text'].values())\n",
    "        transcripts_y.extend(transcripts['sentiment'].values())\n",
    "        \n",
    "#build a pandas df and drop all neutral rows\n",
    "d = {'review': transcripts_x, 'label': transcripts_y}\n",
    "df_transcripts = pd.DataFrame(data=d)\n",
    "\n",
    "#drop all neutral rows\n",
    "df_transcripts = df_transcripts[df_transcripts['label'] != 'neutral']\n",
    "\n",
    "#replace positive and negative to pos and neg\n",
    "df_transcripts['label'] = df_transcripts['label'].str.replace('positive','1')\n",
    "df_transcripts['label'] = df_transcripts['label'].str.replace('negative','0')\n",
    "\n",
    "transcripts_x = df_transcripts['review'].tolist()\n",
    "transcripts_y = df_transcripts['label'].astype('int32').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'll just remind you that the units â€“ those do not count in our unit totals nor do the units from Whole Foods Market. So, yeah, I would say essentially with that backdrop, we're still very, very encouraged by the demand and the reception from customers on the consumer side. We have Amazon fulfilled units are still growing faster than paid units. 3P is now up to 53% of total paid units.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts_x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenize(text):\n",
    "    tokens = text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ')\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_x_tokens = []\n",
    "for t in transcripts_x:\n",
    "    transcripts_x_tokens.append(my_tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_x_index = []\n",
    "for t_tokens in transcripts_x_tokens:\n",
    "    t_index = []\n",
    "    for t in t_tokens:\n",
    "        #get index from word_index for current token\n",
    "        try:\n",
    "            i = word_index[t] \n",
    "        except KeyError:\n",
    "            i = 0\n",
    "        t_index.append(i)\n",
    "    transcripts_x_index.append(t_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_x_index = pad_sequences(transcripts_x_index, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "transcripts_y = to_categorical(np.asarray(transcripts_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  8309,  1291,    22],\n",
       "       [    0,     0,     0, ...,    74,    63,  1060],\n",
       "       [    0,     0,     0, ...,   961,  1536, 16718],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,  5375,  3188,  5235],\n",
       "       [    0,     0,     0, ...,  1787,    52,   943],\n",
       "       [    0,     0,     0, ...,    94,    50,   275]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts_x_index[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (379, 800)\n",
      "Shape of label tensor: (379, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', transcripts_x_index.shape)\n",
    "print('Shape of label tensor:', transcripts_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_prob = model.predict(transcripts_x_index)\n",
    "y_classes = y_prob.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4999622, 0.5000378],\n",
       "       [0.4999622, 0.5000378],\n",
       "       [0.4999622, 0.5000378],\n",
       "       [0.4999622, 0.5000378],\n",
       "       [0.4999622, 0.5000378],\n",
       "       [0.4999622, 0.5000378],\n",
       "       [0.4999622, 0.5000378],\n",
       "       [0.4999622, 0.5000378],\n",
       "       [0.4999622, 0.5000378],\n",
       "       [0.4999622, 0.5000378]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_prob[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_index_2 = {0:'negative',1:'positive',2:'neutral'} \n",
    "def pred_vec_to_lebal(vecs,labels_index_2):\n",
    "    indices = [np.where(r==1)[0][0] for r in vecs]\n",
    "    labels = [labels_index_2[i] for i in indices]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_labels = pred_vec_to_lebal(transcripts_y,labels_index_2)\n",
    "y_classes_labels = pred_vec_to_lebal(y_classes,labels_index_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative',\n",
       " 'negative']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_classes_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         |   n   p |\n",
      "         |   e   o |\n",
      "         |   g   s |\n",
      "         |   a   i |\n",
      "         |   t   t |\n",
      "         |   i   i |\n",
      "         |   v   v |\n",
      "         |   e   e |\n",
      "---------+---------+\n",
      "negative | <62>  . |\n",
      "positive | 317  <.>|\n",
      "---------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "print(ConfusionMatrix(y_val_labels, y_classes_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
