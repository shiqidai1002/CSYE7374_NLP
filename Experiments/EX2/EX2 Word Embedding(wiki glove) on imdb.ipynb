{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "GLOVE_DIR = os.path.join(BASE_DIR, 'glove.6B')\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "MAX_SEQUENCE_LENGTH = 800\n",
    "MAX_NUM_WORDS = 500000\n",
    "EMBEDDING_DIM = 100\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors.\n",
      "Found 400001 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# first, build index mapping words in the embeddings set\n",
    "# to their embedding vector\n",
    "\n",
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, 'glove.6B.100d.txt'),'rb') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "#a = (x_train, y_train), b = (x_test, y_test)\n",
    "a,b = imdb.load_data(path=\"imdb.npz\",\n",
    "                     num_words=None,\n",
    "                     skip_top=10,\n",
    "                     maxlen=MAX_SEQUENCE_LENGTH,\n",
    "                     seed=113,\n",
    "                     start_char=1,\n",
    "                     oov_char=2,\n",
    "                     index_from=3)\n",
    "\n",
    "x_train = a[0]\n",
    "y_train = a[1]\n",
    "x_test = b[0]\n",
    "y_test = b[1]\n",
    "\n",
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 454., 1826., 3691., 5577., 3504., 2246., 1696., 1250., 1000.,\n",
       "         773.,  633.,  509.,  418.,  328.,  287.,  227.,  172.,  155.,\n",
       "         140.,  114.]),\n",
       " array([  9. ,  48.5,  88. , 127.5, 167. , 206.5, 246. , 285.5, 325. ,\n",
       "        364.5, 404. , 443.5, 483. , 522.5, 562. , 601.5, 641. , 680.5,\n",
       "        720. , 759.5, 799. ]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAEhJJREFUeJzt3X+M5HV9x/HnS07wtweyEHoHPYwXI01aJBc8S9NYsPyy8fhDkjOmXg3NJS1NtG1ijzYp8VeCTaPWpGqJXHsaFanVQpAWL4Bp2kTgEEQQ8ValcD3krj3AWqMV++4f81kdcfd29m53ZtjP85FM5vt9z2fm+56d2X3t99dMqgpJUn+eNekGJEmTYQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOrVm0g0czoknnlgbNmyYdBuS9Ixy1113/WdVzSw2bqoDYMOGDezZs2fSbUjSM0qSfx9lnJuAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpU1N9JnCPNuz4/BHf96GrXreMnUha7VwDkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1aqQASPJQkq8muSfJnlY7IcnuJHvb9fGtniQfTDKb5N4kZw09zrY2fm+SbSvzlCRJo1jKGsBvVNWZVbWpze8AbqmqjcAtbR7gImBju2wHPgyDwACuBF4FnA1cORcakqTxO5pNQFuAXW16F3DJUP1jNfAlYG2SU4ALgN1VdaiqHgd2AxcexfIlSUdh1AAo4AtJ7kqyvdVOrqpHAdr1Sa2+Dnhk6L77Wm2huiRpAkb9Uvhzqmp/kpOA3Um+fpixmadWh6n/7J0HAbMd4LTTThuxPUnSUo20BlBV+9v1AeBzDLbhP9Y27dCuD7Th+4BTh+6+Hth/mPrTl3V1VW2qqk0zMzNLezaSpJEtGgBJnp/khXPTwPnAfcANwNyRPNuA69v0DcCb29FAm4En2yaim4Hzkxzfdv6e32qSpAkYZRPQycDnksyN/2RV/XOSO4HrklwGPAxc2sbfBFwMzALfB94CUFWHkrwLuLONe2dVHVq2ZyJJWpJFA6CqvgX8yjz1/wLOm6dewOULPNZOYOfS25QkLTfPBJakThkAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6tTIAZDkmCR3J7mxzZ+e5PYke5N8OsmxrX5cm59tt28YeowrWv3BJBcs95ORJI1uKWsAbwUeGJp/L/D+qtoIPA5c1uqXAY9X1cuA97dxJDkD2Ar8EnAh8KEkxxxd+5KkIzVSACRZD7wO+GibD3Au8Jk2ZBdwSZve0uZpt5/Xxm8Brq2qH1bVt4FZ4OzleBKSpKUbdQ3gA8Dbgf9r8y8Bnqiqp9r8PmBdm14HPALQbn+yjf9JfZ77/ESS7Un2JNlz8ODBJTwVSdJSLBoASX4LOFBVdw2X5xlai9x2uPv8tFB1dVVtqqpNMzMzi7UnSTpCa0YYcw7w+iQXA88BXsRgjWBtkjXtv/z1wP42fh9wKrAvyRrgxcChofqc4ftIksZs0TWAqrqiqtZX1QYGO3Fvrao3AbcBb2jDtgHXt+kb2jzt9lurqlp9aztK6HRgI3DHsj0TSdKSjLIGsJA/Aa5N8m7gbuCaVr8G+HiSWQb/+W8FqKr7k1wHfA14Cri8qn58FMuXJB2FJQVAVX0R+GKb/hbzHMVTVT8ALl3g/u8B3rPUJiVJy88zgSWpUwaAJHXKAJCkThkAktQpA0CSOmUASFKnjuY8AC1gw47PT7oFSVqUawCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUx4Guooc7eGnD131umXqRNIzgWsAktQpA0CSOmUASFKnDABJ6pQBIEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTiwZAkuckuSPJV5Lcn+QdrX56ktuT7E3y6STHtvpxbX623b5h6LGuaPUHk1ywUk9KkrS4UdYAfgicW1W/ApwJXJhkM/Be4P1VtRF4HLisjb8MeLyqXga8v40jyRnAVuCXgAuBDyU5ZjmfjCRpdIsGQA18r80+u10KOBf4TKvvAi5p01vaPO3285Kk1a+tqh9W1beBWeDsZXkWkqQlG2kfQJJjktwDHAB2A98Enqiqp9qQfcC6Nr0OeASg3f4k8JLh+jz3GV7W9iR7kuw5ePDg0p+RJGkkIwVAVf24qs4E1jP4r/0V8w1r11ngtoXqT1/W1VW1qao2zczMjNKeJOkILOkooKp6AvgisBlYm2TuO4XXA/vb9D7gVIB2+4uBQ8P1ee4jSRqzUY4Cmkmytk0/F3gt8ABwG/CGNmwbcH2bvqHN026/taqq1be2o4ROBzYCdyzXE5EkLc2axYdwCrCrHbHzLOC6qroxydeAa5O8G7gbuKaNvwb4eJJZBv/5bwWoqvuTXAd8DXgKuLyqfry8T0eSNKpFA6Cq7gVeOU/9W8xzFE9V/QC4dIHHeg/wnqW3KUlabp4JLEmdMgAkqVMGgCR1ygCQpE4ZAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXKAJCkThkAktQpA0CSOrVm0g1oemzY8fkjvu9DV71uGTuRNA6uAUhSpwwASeqUASBJnTIAJKlTBoAkdWrRAEhyapLbkjyQ5P4kb231E5LsTrK3XR/f6knywSSzSe5NctbQY21r4/cm2bZyT0uStJhR1gCeAv64ql4BbAYuT3IGsAO4pao2Are0eYCLgI3tsh34MAwCA7gSeBVwNnDlXGhIksZv0QCoqker6stt+r+BB4B1wBZgVxu2C7ikTW8BPlYDXwLWJjkFuADYXVWHqupxYDdw4bI+G0nSyJa0DyDJBuCVwO3AyVX1KAxCAjipDVsHPDJ0t32ttlBdkjQBIwdAkhcA/wC8raq+e7ih89TqMPWnL2d7kj1J9hw8eHDU9iRJSzRSACR5NoM//p+oqs+28mNt0w7t+kCr7wNOHbr7emD/Yeo/o6qurqpNVbVpZmZmKc9FkrQEoxwFFOAa4IGqet/QTTcAc0fybAOuH6q/uR0NtBl4sm0iuhk4P8nxbefv+a0mSZqAUT4M7hzgt4GvJrmn1f4UuAq4LsllwMPApe22m4CLgVng+8BbAKrqUJJ3AXe2ce+sqkPL8iwkSUu2aABU1b8y//Z7gPPmGV/A5Qs81k5g51IalCStDM8ElqRO+X0ACziaz8aXpGcC1wAkqVMGgCR1ygCQpE4ZAJLUKXcCa1n4hfLSM49rAJLUKQNAkjplAEhSpwwASeqUASBJnTIAJKlTBoAkdcoAkKROGQCS1CkDQJI6ZQBIUqcMAEnqlAEgSZ0yACSpUwaAJHXK7wPQxPldAtJkuAYgSZ0yACSpUwaAJHXKAJCkThkAktSpRQMgyc4kB5LcN1Q7IcnuJHvb9fGtniQfTDKb5N4kZw3dZ1sbvzfJtpV5OpKkUY2yBvB3wIVPq+0AbqmqjcAtbR7gImBju2wHPgyDwACuBF4FnA1cORcakqTJWDQAqupfgENPK28BdrXpXcAlQ/WP1cCXgLVJTgEuAHZX1aGqehzYzc+HiiRpjI50H8DJVfUoQLs+qdXXAY8MjdvXagvVJUkTstxnAmeeWh2m/vMPkGxnsPmI0047bfk606p0NGcRg2cSq29HugbwWNu0Q7s+0Or7gFOHxq0H9h+m/nOq6uqq2lRVm2ZmZo6wPUnSYo40AG4A5o7k2QZcP1R/czsaaDPwZNtEdDNwfpLj287f81tNkjQhi24CSvIp4DXAiUn2MTia5yrguiSXAQ8Dl7bhNwEXA7PA94G3AFTVoSTvAu5s495ZVU/fsSxJGqNFA6Cq3rjATefNM7aAyxd4nJ3AziV1J0laMZ4JLEmdMgAkqVMGgCR1ym8EU9f8NjL1zDUASeqUASBJnTIAJKlTBoAkdcqdwNIRcgeynulcA5CkThkAktQpA0CSOmUASFKn3AksTYA7kDUNXAOQpE6t6jWAo/2+WElazVZ1AEirkZuPtFzcBCRJnTIAJKlTBoAkdcp9AFJHjvbACPchrC4GgKSRuQN6dXETkCR1yjUASWPh2sP0cQ1AkjrlGoCkqefaw8owACStaobHwgwASVrAaj9sduwBkORC4K+AY4CPVtVV4+5BksZh2tc+xroTOMkxwF8DFwFnAG9McsY4e5AkDYz7KKCzgdmq+lZV/S9wLbBlzD1Ikhh/AKwDHhma39dqkqQxG/c+gMxTq58ZkGwHtrfZ7yV5cAmPfyLwn0fY20qb1t6mtS+Y3t6mtS+Y3t6mtS+Y0t7y3qPq6xdHGTTuANgHnDo0vx7YPzygqq4Grj6SB0+yp6o2HXl7K2dae5vWvmB6e5vWvmB6e5vWvmB6extHX+PeBHQnsDHJ6UmOBbYCN4y5B0kSY14DqKqnkvwBcDODw0B3VtX94+xBkjQw9vMAquom4KYVevgj2nQ0JtPa27T2BdPb27T2BdPb27T2BdPb24r3lapafJQkadXx00AlqVOrJgCSXJjkwSSzSXaMedk7kxxIct9Q7YQku5PsbdfHt3qSfLD1eW+Ss1awr1OT3JbkgST3J3nrFPX2nCR3JPlK6+0drX56kttbb59uBwuQ5Lg2P9tu37BSvbXlHZPk7iQ3TllfDyX5apJ7kuxptYm/nm15a5N8JsnX23vu1ZPuLcnL289q7vLdJG+bdF9D/f1he//fl+RT7fdifO+1qnrGXxjsUP4m8FLgWOArwBljXP6vA2cB9w3V/gLY0aZ3AO9t0xcD/8TgnIjNwO0r2NcpwFlt+oXANxh8BMc09BbgBW362cDtbZnXAVtb/SPA77Xp3wc+0qa3Ap9e4df0j4BPAje2+Wnp6yHgxKfVJv56tuXtAn63TR8LrJ2W3toyjwG+w+AY+Yn3xeAk2G8Dzx16j/3OON9rK/oDH9cFeDVw89D8FcAVY+5hAz8bAA8Cp7TpU4AH2/TfAG+cb9wYerwe+M1p6w14HvBl4FUMTnxZ8/TXlcGRY69u02vauKxQP+uBW4BzgRvbH4OJ99WW8RA/HwATfz2BF7U/Zpm23oaWcT7wb9PSFz/9ZIQT2nvnRuCCcb7XVssmoGn8iImTq+pRgHZ9UqtPpNe2uvhKBv9pT0VvbTPLPcABYDeDtbgnquqpeZb/k97a7U8CL1mh1j4AvB34vzb/kinpCwZnzn8hyV0ZnDUP0/F6vhQ4CPxt23T20STPn5Le5mwFPtWmJ95XVf0H8JfAw8CjDN47dzHG99pqCYBFP2Jiioy91yQvAP4BeFtVffdwQ+eprVhvVfXjqjqTwX/cZwOvOMzyx9Jbkt8CDlTVXcPlSfc15JyqOovBJ+penuTXDzN2nL2tYbAZ9MNV9UrgfxhsWlnIWH9ubTv664G/X2zoPLUV6avtd9gCnA78AvB8Bq/rQstf9t5WSwAs+hETE/BYklMA2vWBVh9rr0mezeCP/yeq6rPT1NucqnoC+CKDba5rk8ydnzK8/J/01m5/MXBoBdo5B3h9kocYfFrtuQzWCCbdFwBVtb9dHwA+xyA4p+H13Afsq6rb2/xnGATCNPQGgz+sX66qx9r8NPT1WuDbVXWwqn4EfBb4Vcb4XlstATCNHzFxA7CtTW9jsP19rv7mdrTBZuDJuVXR5ZYkwDXAA1X1vinrbSbJ2jb9XAa/DA8AtwFvWKC3uZ7fANxabWPocqqqK6pqfVVtYPA+urWq3jTpvgCSPD/JC+emGWzTvo8peD2r6jvAI0le3krnAV+bht6aN/LTzT9zy590Xw8Dm5M8r/2uzv3MxvdeW8mdLuO8MNh7/w0G25H/bMzL/hSDbXg/YpDSlzHYNncLsLddn9DGhsGX4nwT+CqwaQX7+jUGq4j3Ave0y8VT0tsvA3e33u4D/rzVXwrcAcwyWF0/rtWf0+Zn2+0vHcPr+hp+ehTQxPtqPXylXe6fe59Pw+vZlncmsKe9pv8IHD8NvTE4yOC/gBcP1SbeV1veO4Cvt9+BjwPHjfO95pnAktSp1bIJSJK0RAaAJHXKAJCkThkAktQpA0CSOmUASFKnDABJ6pQBIEmd+n8D4EdG9tP6nwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot the lengths distribution when the max len set as 1000\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "p_lengths = []\n",
    "for p in x_train:\n",
    "    p_lengths.append(len(p))\n",
    "\n",
    "plt.hist(p_lengths,bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25000, 800)\n",
      "Shape of label tensor: (25000, 2)\n"
     ]
    }
   ],
   "source": [
    "x_train = pad_sequences(x_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(x_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "y_train = to_categorical(np.asarray(y_train))\n",
    "y_test = to_categorical(np.asarray(y_test))\n",
    "\n",
    "print('Shape of data tensor:', x_train.shape)\n",
    "print('Shape of label tensor:', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index)) + 4\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 800, 100)          8858800   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_9 ( (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                1616      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 8,860,450\n",
      "Trainable params: 1,650\n",
      "Non-trainable params: 8,858,800\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88584"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 24067 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 12s 482us/step - loss: 0.6932 - acc: 0.5005 - val_loss: 0.6932 - val_acc: 0.5002\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 9s 360us/step - loss: 0.6932 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 9s 356us/step - loss: 0.6932 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 9s 374us/step - loss: 0.6932 - acc: 0.4990 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 10s 402us/step - loss: 0.6932 - acc: 0.4988 - val_loss: 0.6932 - val_acc: 0.5002\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 9s 367us/step - loss: 0.6932 - acc: 0.4989 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 9s 378us/step - loss: 0.6932 - acc: 0.4977 - val_loss: 0.6932 - val_acc: 0.5002\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 9s 355us/step - loss: 0.6932 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 9s 362us/step - loss: 0.6932 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.5002\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 9s 355us/step - loss: 0.6932 - acc: 0.5011 - val_loss: 0.6931 - val_acc: 0.4998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16460cc2d30>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('EX2_WordEmbedding_trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "# second, prepare text samples and their labels\n",
    "print('Processing text dataset')\n",
    "\n",
    "transcripts_x = []\n",
    "transcripts_y = []\n",
    "\n",
    "file_list = os.listdir('data')\n",
    "for file in file_list:\n",
    "    with open('data/' + file, 'r') as f:\n",
    "        transcripts = json.load(f)\n",
    "        transcripts_x.extend(transcripts['text'].values())\n",
    "        transcripts_y.extend(transcripts['sentiment'].values())\n",
    "        \n",
    "#build a pandas df and drop all neutral rows\n",
    "d = {'review': transcripts_x, 'label': transcripts_y}\n",
    "df_transcripts = pd.DataFrame(data=d)\n",
    "\n",
    "#drop all neutral rows\n",
    "df_transcripts = df_transcripts[df_transcripts['label'] != 'neutral']\n",
    "\n",
    "#replace positive and negative to pos and neg\n",
    "df_transcripts['label'] = df_transcripts['label'].str.replace('positive','1')\n",
    "df_transcripts['label'] = df_transcripts['label'].str.replace('negative','0')\n",
    "\n",
    "transcripts_x = df_transcripts['review'].tolist()\n",
    "transcripts_y = df_transcripts['label'].astype('int32').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'll just remind you that the units â€“ those do not count in our unit totals nor do the units from Whole Foods Market. So, yeah, I would say essentially with that backdrop, we're still very, very encouraged by the demand and the reception from customers on the consumer side. We have Amazon fulfilled units are still growing faster than paid units. 3P is now up to 53% of total paid units.\""
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts_x[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenize(text):\n",
    "    tokens = text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ')\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_x_tokens = []\n",
    "for t in transcripts_x:\n",
    "    transcripts_x_tokens.append(my_tokenize(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_x_index = []\n",
    "for t_tokens in transcripts_x_tokens:\n",
    "    t_index = []\n",
    "    for t in t_tokens:\n",
    "        #get index from word_index for current token\n",
    "        try:\n",
    "            i = word_index[t] \n",
    "        except KeyError:\n",
    "            i = 0\n",
    "        t_index.append(i)\n",
    "    transcripts_x_index.append(t_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_x_index = pad_sequences(transcripts_x_index, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "transcripts_y = to_categorical(np.asarray(transcripts_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (379, 800)\n",
      "Shape of label tensor: (379, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of data tensor:', transcripts_x_index.shape)\n",
    "print('Shape of label tensor:', transcripts_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_prob = model.predict(transcripts_x_index)\n",
    "y_classes = y_prob.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_index_2 = {0:'negative',1:'positive',2:'neutral'} \n",
    "def pred_vec_to_lebal(vecs,labels_index_2):\n",
    "    indices = [np.where(r==1)[0][0] for r in vecs]\n",
    "    labels = [labels_index_2[i] for i in indices]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_labels = pred_vec_to_lebal(transcripts_y,labels_index_2)\n",
    "y_classes_labels = pred_vec_to_lebal(y_classes,labels_index_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         |   n   p |\n",
      "         |   e   o |\n",
      "         |   g   s |\n",
      "         |   a   i |\n",
      "         |   t   t |\n",
      "         |   i   i |\n",
      "         |   v   v |\n",
      "         |   e   e |\n",
      "---------+---------+\n",
      "negative | <62>  . |\n",
      "positive | 317  <.>|\n",
      "---------+---------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics import ConfusionMatrix\n",
    "print(ConfusionMatrix(y_val_labels, y_classes_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
