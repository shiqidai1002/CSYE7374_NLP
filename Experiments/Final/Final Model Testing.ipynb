{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Obtain the earnings call transcript for Tesla.\n",
    "2. Parse and format the file in the json format described before\n",
    "3. Each team should read the transcript and label the paragraphs with (+/-/neutral)\n",
    "sentiments.\n",
    "4. Pass this file to the final model you selected. How does it perform (Show confusion\n",
    "matrix)?\n",
    "5. On November 25th, share your labeled json for the Tesla report with the class.\n",
    "6. Pass the 4 other files (from other teams) to your model. How does it perform (Show\n",
    "confusion matrix)?\n",
    "7. Discuss what you learnt from this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, GlobalAveragePooling1D, LSTM\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model, load_model\n",
    "from keras.models import Sequential\n",
    "from keras.initializers import Constant\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = ''\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'data')\n",
    "MAX_SEQUENCE_LENGTH = 800\n",
    "\n",
    "def test_on_transcripts(filepath):\n",
    "    #get imdb word index\n",
    "    from keras.datasets import imdb\n",
    "    word_index = imdb.get_word_index()\n",
    "\n",
    "\n",
    "    #load pre-trained model(complied already)\n",
    "    model = load_model('EX2_RNN(LSTM)_trained_model')\n",
    "\n",
    "    #Load data    \n",
    "    def load_data_from_csv(filepath):\n",
    "        df_transcripts = pd.read_csv(filepath)\n",
    "\n",
    "        transcripts_x = []\n",
    "        transcripts_y = []\n",
    "\n",
    "        #drop all neutral rows\n",
    "        df_transcripts = df_transcripts[df_transcripts['sentiment'] != 'neutral']\n",
    "\n",
    "        #replace positive and negative to pos and neg\n",
    "        df_transcripts['sentiment'] = df_transcripts['sentiment'].str.replace('positive','1')\n",
    "        df_transcripts['sentiment'] = df_transcripts['sentiment'].str.replace('negative','0')\n",
    "\n",
    "        transcripts_x = df_transcripts['text'].tolist()\n",
    "        transcripts_y = df_transcripts['sentiment'].astype('int32').tolist()\n",
    "\n",
    "        return transcripts_x, transcripts_y\n",
    "\n",
    "\n",
    "    #get list of x and list of y\n",
    "    transcripts_x, transcripts_y = load_data_from_csv(filepath)\n",
    "\n",
    "\n",
    "    #define a tokenizer\n",
    "    def my_tokenize(text):\n",
    "        tokens = text_to_word_sequence(text, \n",
    "                                       filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', \n",
    "                                       lower=True, #all words in word_index are in lower case\n",
    "                                       split=' ') \n",
    "        return tokens\n",
    "\n",
    "\n",
    "    #tokenize x\n",
    "    transcripts_x_tokens = []\n",
    "    for t in transcripts_x:\n",
    "        transcripts_x_tokens.append(my_tokenize(t))\n",
    "\n",
    "    # convert words to indices    \n",
    "    transcripts_x_index = []\n",
    "    for t_tokens in transcripts_x_tokens:\n",
    "        t_index = []\n",
    "        for t in t_tokens:\n",
    "            #get index from word_index for current token\n",
    "            try:\n",
    "                i = word_index[t] \n",
    "            except KeyError:\n",
    "                i = 0\n",
    "            t_index.append(i)\n",
    "        transcripts_x_index.append(t_index)\n",
    "\n",
    "\n",
    "    #padding and to_categorical\n",
    "    transcripts_x_index = pad_sequences(transcripts_x_index, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "    transcripts_y = to_categorical(np.asarray(transcripts_y))\n",
    "\n",
    "\n",
    "    # Predicting the Test set results\n",
    "    y_prob = model.predict(transcripts_x_index)\n",
    "    y_classes = y_prob.argmax(axis=-1)\n",
    "\n",
    "\n",
    "    #post processing\n",
    "\n",
    "    labels_index_2 = {0:'negative',1:'positive',2:'neutral'} \n",
    "    def pred_vec_to_lebal(vec,labels_index_2):\n",
    "        indices = [np.where(r==1)[0][0] for r in vec]\n",
    "        labels = [labels_index_2[i] for i in indices]\n",
    "        return labels\n",
    "    y_val_labels = pred_vec_to_lebal(transcripts_y,labels_index_2)\n",
    "\n",
    "    def pred_vec_to_lebal2(vec,labels_index_2):\n",
    "        labels = [labels_index_2[i] for i in vec]\n",
    "        return labels\n",
    "    y_classes_labels = pred_vec_to_lebal2(y_classes,labels_index_2)\n",
    "\n",
    "\n",
    "    #Evaluation\n",
    "    import sklearn.metrics\n",
    "    cm = sklearn.metrics.confusion_matrix(y_val_labels, y_classes_labels, labels=[\"positive\", \"negative\"])\n",
    "    print(\"Confusion Matrix: \")\n",
    "    print(\"|TP:\"+ str(cm[0][0]) + \" | FN:\"+str(cm[0][1])+'|')\n",
    "    print(\"|FP:\"+ str(cm[1][0]) + \" | TN:\"+str(cm[1][1])+'|')\n",
    "    precision = sklearn.metrics.precision_score(y_val_labels, y_classes_labels, average='weighted')\n",
    "    recall = sklearn.metrics.recall_score(y_val_labels, y_classes_labels, average='weighted')\n",
    "    accuracy = sklearn.metrics.accuracy_score(y_val_labels, y_classes_labels)\n",
    "    print()\n",
    "    print(\"Accuracy: \" + str(accuracy))\n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_base_path = 'test data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tesla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "|TP:54 | FN:49|\n",
      "|FP:18 | TN:13|\n",
      "\n",
      "Accuracy: 0.5\n",
      "Precision: 0.625\n",
      "Recall: 0.5\n"
     ]
    }
   ],
   "source": [
    "tesla_path = test_data_base_path + 'TESLA_earnings_call_transcript.csv'\n",
    "\n",
    "test_on_transcripts(tesla_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "|TP:43 | FN:33|\n",
      "|FP:6 | TN:3|\n",
      "\n",
      "Accuracy: 0.5411764705882353\n",
      "Precision: 0.7934573829531812\n",
      "Recall: 0.5411764705882353\n"
     ]
    }
   ],
   "source": [
    "google_path = test_data_base_path + 'google.csv'\n",
    "test_on_transcripts(google_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "|TP:12 | FN:10|\n",
      "|FP:1 | TN:3|\n",
      "\n",
      "Accuracy: 0.5769230769230769\n",
      "Precision: 0.8165680473372782\n",
      "Recall: 0.5769230769230769\n"
     ]
    }
   ],
   "source": [
    "amazon_path = test_data_base_path +'amazon.csv'\n",
    "test_on_transcripts(amazon_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Netflix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "|TP:38 | FN:23|\n",
      "|FP:6 | TN:7|\n",
      "\n",
      "Accuracy: 0.6081081081081081\n",
      "Precision: 0.7529074529074529\n",
      "Recall: 0.6081081081081081\n"
     ]
    }
   ],
   "source": [
    "netflix_path = test_data_base_path + 'netflix.csv'\n",
    "test_on_transcripts(netflix_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microsoft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix: \n",
      "|TP:55 | FN:36|\n",
      "|FP:2 | TN:1|\n",
      "\n",
      "Accuracy: 0.5957446808510638\n",
      "Precision: 0.9349797726057524\n",
      "Recall: 0.5957446808510638\n"
     ]
    }
   ],
   "source": [
    "microsoft_path = test_data_base_path + 'microsoft.csv'\n",
    "test_on_transcripts(microsoft_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
